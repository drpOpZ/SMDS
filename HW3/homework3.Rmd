---
title: "Homework 3"
author: "Group F - Claudia Dorigo, Rossella Marvulli, Michele Rispoli, Sebastiano Zagatti"
date: "2020/05/13"
output:
  html_document:
    toc: yes
    fig_caption: yes
  beamer_presentation:
    highlight: tango
  include: null
  ioslides_presentation:
    highlight: tango
  pdf_document:
    highlight: tango
    keep_tex: yes
    toc: yes
  slide_level: 2
  slidy_presentation:
    fig.height: 3
    fig.width: 4
    highlight: tango
header-includes:
- \usepackage{color}
- \definecolor{Purple}{HTML}{911146}
- \definecolor{Orange}{HTML}{CF4A30}
- \setbeamercolor{alerted text}{fg=Orange}
- \setbeamercolor{frametitle}{bg=Purple}
institute: University of Udine & University of Trieste
graphics: yes
subtitle: "Deadline: 2020/05/13"
fontsize: 10pt
---
```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.align = 'center', warning=FALSE, message=FALSE, fig.asp=0.625, dev='png', global.par = TRUE, dev.args=list(pointsize=10), fig.path = 'figs/')
library(MASS)
library(DAAG)
```
```{r setup, include=FALSE}
library(knitr)
local({
  hook_plot = knit_hooks$get('plot')
  knit_hooks$set(plot = function(x, options) {
    paste0('\n\n----\n\n', hook_plot(x, options))
  })
})
```

# Exercises from LAB

## Ex. 1
Given the following samples $y_1,...,y_{15}$ of a R.V. $Y \sim \text{Weibull}(\gamma,\beta)$
```R
y <- c(155.9, 200.2, 143.8, 150.1,152.1, 142.2, 147, 146, 146,
 170.3, 148, 140, 118, 144, 97)
```
consider the corresponding MLE $\hat \theta =(\hat \gamma, \hat \beta)$ of the model's parameters.

Use `nlm` to compute the variance for the estimator $\hat w = (\log \hat \gamma , \log \hat \beta)$ and `optimHess` for the variance of $\hat \theta =(\hat \gamma, \hat \beta)$.

---

We know from the theory that we may estimate the variance of the estimators of our model's parameter by looking at the (diagonal of the) inverse of the *information matrix* (i.e. the Hessian matrix of the log-likelihood) evaluated  at the MLE, that is
$$
\text{Var}(  \mathbf{\hat \theta}) \approx \text{diag}(J^{-1}(\mathbf{\hat \theta}))
$$

In our case we wish to estimate these values for the logarithmic transform of the parameters and the parameters themselves.

```{r lab1, echo=TRUE}

#data
y <- c(155.9, 200.2, 143.8, 150.1,152.1, 142.2, 147, 146, 146,
 170.3, 148, 140, 118, 144, 97)

#negative log likelihood
negloglkh <- function(data, param){
  -sum(dweibull(data, 
                shape = param[1], 
                scale = param[2],
                log = TRUE))
}

#negative log likelihood with transformed parameters
negloglkh.rep <- function(data, param) negloglkh(data, exp(param))

#compute jhat.w using nlm
optimized.nll.rep<-nlm(negloglkh.rep,
                       c(0,0),
                       hessian=T,
                       data=y)

#retrieve the information matrix for w
jhat.w <- optimized.nll.rep$hessian
print(jhat.w)

#retrieve variances for w
var.w.hat <- diag(solve(jhat.w))
print(var.w.hat)


#retrieve theta.hat by transforming the estimate just found
theta.hat <- exp(optimized.nll.rep$estimate)
print(theta.hat)

#compute jhat using optimHess evaluated on theta.hat 
jhat <-optimHess(theta.hat, negloglkh, data=y)
print(jhat)

#retrieve variances for theta.hat
var.theta.hat <- diag(solve(jhat))
print(var.theta.hat)

```

## Ex. 2

The Wald confidence interval with level $1-\alpha$ is defined as:
$$\widehat{\gamma}\pm z_{1-\alpha/2}j_P(\widehat{\gamma})^{-1/2}$$

Compute the Wald confidence interval of level 0.95 and plot the results.

---

```{r lab2, echo=TRUE}
y <- c(155.9, 200.2, 143.8, 150.1,152.1, 142.2, 147, 146, 146,
       170.3, 148, 140, 118, 144, 97)
n <- length(y)
log_lik_weibull <- function( data, param){
  -sum(dweibull(data, shape = param[1], scale = param[2], log = TRUE))
}

log_lik_weibull_profile  <- function(data, gamma){
  beta.gamma <- mean(data^gamma)^(1/gamma)
  log_lik_weibull( data, c(gamma, beta.gamma) )
}

weib.y.mle<-optim(c(1,1),fn=log_lik_weibull,hessian=T,
                  method='L-BFGS-B',lower=rep(1e-7,2),
                  upper=rep(Inf,2),data=y)

estim <- weib.y.mle$par
gammahat <- estim[1]
betahat <- estim[2]

jhat<-matrix(NA,nrow=2,ncol=2)
jhat[1,1]<-n/gammahat^2+sum((y/betahat)^gammahat*
                              (log(y/betahat))^2)
jhat[1,2]<-jhat[2,1]<- n/betahat-sum(y^gammahat/betahat^(gammahat+1)*
                                       (gammahat*log(y/betahat)+1))
jhat[2,2]<- -n*gammahat/betahat^2+gammahat*(gammahat+1)/
  betahat^(gammahat+2)*sum(y^gammahat)
solve(jhat)

mle.se<-sqrt(diag(solve(jhat)))

log_lik_weibull_profile_v <-Vectorize(log_lik_weibull_profile, 'gamma'  )

plot(function(x) -log_lik_weibull_profile_v(data=y, x)+weib.y.mle$value,
     from=0.1,to=15,xlab=expression(gamma),
     ylab='profile relative log likelihood',ylim=c(-8,0))
conf.level<-0.975

lower <- gammahat-qnorm(conf.level,mean=0,sd=1)*sqrt(mle.se[1])
upper <- gammahat+qnorm(conf.level,mean=0,sd=1)*sqrt(mle.se[1])

segments( lower,-log_lik_weibull_profile_v(data=y, lower)+weib.y.mle$value, lower, -log_lik_weibull_profile_v(y, lower), col="red", lty=2  )
segments( upper,-log_lik_weibull_profile_v(data=y, upper)+weib.y.mle$value, upper, -log_lik_weibull_profile_v(y, upper), col="red", lty=2  )
segments( lower,
          -8.1, upper,
          -8.1, col="red", lty =1, lwd=2  )
text(7,-7.5,"95% Wald CI",col=2)
```

## Ex. 3

  Repeat the steps above —write the profile log-likelihood, plot it and find the deviance confidence intervals— considering this time $\gamma$ as a nuisance parameter and $\beta$ as the parameter of interest.

---

  I can treat $\beta$ as the parameter of interest and $\gamma$ as the nuisance parameter. We may then define the profile log-likelihood:
$$
  l_p(\beta)=\underset{\gamma}{max} l(\gamma,\beta,y)=l(\hat{\gamma_{\beta}},\beta,y)
$$
where $\hat{\gamma_{\beta}}$ is the constrained MLE for $\gamma$ with $\beta$ fixed.

First of all I plot the contour plot of the log likelihood function and then I plot a red dashed line which corresponds to the profile log likelihood defined above.

In the case in which $\gamma$ was the parameter of interest I was able, equating to 0 the derivative of the log likelihood wrt $\beta$, to find an analytical formula to compute $\beta$ as function of $\gamma$.

  But now that $\beta$ is the parameter of interest I cannot find a close formula to compute $\gamma$ analytically having $\beta$ from the partial derivative of the log likelihood w.r.t $\gamma$. Hence I've created a function `deriv()` which computes the derivative $\frac{\delta}{\delta\gamma}l(\gamma,\beta,y)$ and another function `gamma.beta_fun` which given a value for $\beta$ computes numerically a zero of the `deriv()` function which now depends only on $\gamma$.
  
```{r lab3.1, echo=TRUE}
# data 
y <- c(155.9, 200.2, 143.8, 150.1,152.1, 142.2, 147, 146, 146, 170.3, 148, 140, 118, 144, 97)
n <- length(y)

# log likelihood
log_lik_weibull <- function( data, param){
  -sum(dweibull(data, shape = param[1], scale = param[2], log = TRUE))
}
  
#mle
weib.y.mle<-optim(c(1,1),fn=log_lik_weibull,hessian=T,
                    method='L-BFGS-B',lower=rep(1e-7,2),
                    upper=rep(Inf,2),data=y)
  
# prepare the grid to plot log likelihood
gamma <- seq(0.1, 15, length=100)
beta <- seq(100,200, length=100)
parvalues <- expand.grid(gamma,beta)
llikvalues <- apply(parvalues, 1, log_lik_weibull, data=y)
llikvalues <- matrix(-llikvalues, nrow=length(beta), ncol=length(gamma),byrow=T)
conf.levels <- c(0,0.5,0.75,0.9,0.95,0.99)


contour(beta, gamma, llikvalues-max(llikvalues),
        levels=-qchisq(conf.levels, 2)/2,
        xlab=expression(beta),
        labels=as.character(conf.levels),
        ylab=expression(gamma),
        xlim=c(120,190))
title('Weibull profile log-likelihood')

# derivative wrt gamma
deriv <- function(x,beta){
  return(n/x-n*log(beta)+sum(log(y))-sum((y/beta)^x*log(y/beta)))
}
deriv_v <- Vectorize(deriv, "x")

gamma.beta_fun <- function(beta){
  return(uniroot(deriv_v,c(1e-15,15),beta=beta)$root)
} 

gamma.beta <- sapply(beta,gamma.beta_fun)

lines(beta,gamma.beta,col=2,lty=2)
points(weib.y.mle$par[2],weib.y.mle$par[1],pch=19)
```

I can see that also in this case the profile log likelihood passes through $(\hat{\beta}_{ML},\hat{\gamma}_{ML})$.

Now I can plot deviance confidence intervals knowing the asymptotic distribution for the profile likelihood-ratio test statistic:
$$
W_p(\beta)=2\{l_p(\hat{\beta})-l_p(\beta)\} \sim \mathcal{X}^2_1
$$
```{r lab3.2, echo=TRUE}
log_lik_weibull_profile  <- function(data, beta){
  gamma.beta <- gamma.beta_fun(beta)
  log_lik_weibull( data, c(gamma.beta, beta) )
}
log_lik_weibull_profile_v <-Vectorize(log_lik_weibull_profile, 'beta'  )

plot(function(x) -log_lik_weibull_profile_v(data=y, x)+weib.y.mle$value,
     from=120,to=200,xlab=expression(beta),
     ylab='profile relative log likelihood',ylim=c(-8,0))

conf.level<-0.95
abline(h=-qchisq(conf.level,1)/2,lty='dashed',col=2)

conf.level<-0.95
lrt.ci1 <- uniroot(function(x) -log_lik_weibull_profile_v(y, x)+weib.y.mle$value+
                   qchisq(conf.level,1)/2,c(1e-7,weib.y.mle$par[2]))$root
lrt.ci1<-c(lrt.ci1,uniroot(function(x) -log_lik_weibull_profile_v(y,x)+weib.y.mle$value+
                             qchisq(conf.level,1)/2,c(weib.y.mle$par[2],200))$root)
segments( lrt.ci1[1],-qchisq(conf.level,1)/2, lrt.ci1[1],
          -log_lik_weibull_profile_v(y, lrt.ci1[1]), col="red", lty=2  )
segments( lrt.ci1[2],-qchisq(conf.level,1)/2, lrt.ci1[2],
          -log_lik_weibull_profile_v(y, lrt.ci1[2]), col="red", lty=2  )
points(lrt.ci1[1], -qchisq(0.95,1)/2, pch=16, col=2, cex=1.5)
points(lrt.ci1[2], -qchisq(0.95,1)/2, pch=16, col=2, cex=1.5)
segments( lrt.ci1[1],
          -8.1, lrt.ci1[2],
          -8.1, col="red", lty =1, lwd=2  )
text(156,-7,"95% Deviance CI",col=2)
```

## Ex. 5

In `sim` in the code above, you find the MCMC output which allows to approximate the posterior distribution of our parameter of interest with $S$ draws of $\theta$. Please, produce an histogram for these random draws $\theta(1),…,\theta(S)$, compute the empirical quantiles, and overlap the true posterior distribution.

---

We suppose $y_1,...,y_n\sim\mathcal{N(\theta,\sigma^2)}$, with $\sigma^2$ known. We choose a normal prior distribution for theta: $\mathcal{N}(\mu,\tau^2)$, and we know that the posterior distribution of theta will be proportional to a normal distribution with new parameters:
$$\mu^*=\frac{\frac{n}{\sigma^2}\bar{y}+\frac{1}{\tau^2}\mu}{\frac{n}{\sigma^2}+\frac{1}{\tau^2}}$$
$$\tau^{*2}=\Big(\frac{n}{\sigma^2}+\frac{1}{\tau^2}\Big)^{-1}$$

First of all I generate some data fixing the parameters:
```{r lab5.1, echo=TRUE}
library(rstan)
rstan_options(auto_write = TRUE)
set.seed(123)

# parameters
theta_sample <- 2   # true mean
sigma2 <- 2         # known variance
n <- 10             #sample size

#prior parameters
mu <- 7
tau2 <- 2

#generate data
y <- rnorm(n,theta_sample, sqrt(sigma2))
```

Now I can use `stan` to simulate values from the posterior distribution:

```{r lab5.2, echo=TRUE}
data<- list(N=n, y=y, sigma =sqrt(sigma2), mu = mu, tau = sqrt(tau2))
fit <- stan(file="3/normal.stan", data = data, chains = 4, iter=2000)

sim <- extract(fit)
```

I can plot the histogram of the simulated values of $\theta$ and since I know the true value of the parameters that have generated the data, I can compute the posterior parameters and plot also the true posterior distribution:

```{r lab5.3, echo=TRUE}
#posterior parameters
mu_star <- ((1/tau2)*mu+(n/sigma2)*mean(y))/( (1/tau2)+(n/sigma2))
sd_star <- sqrt(1/( (1/tau2)+(n/sigma2)))

# simulated theta
sample <- sim$theta

{
hist(sample,probability = TRUE,breaks=20,xlab="theta",main="posterior simulation")
curve(dnorm(x, mu_star, sd_star), 
      xlab=expression(theta), ylab="", col="blue", lwd=2,
      cex.lab=2, add=T)
legend("topright", "true posterior",lty=1, col="blue",lwd=2)
}
```
Now I want to compute the empirical quantiles and compare them with the quantiles of the true posterior distribution. I compute the empirical quantiles and the theoretical ones (the quantiles of a normal $\mathcal{N}(\mu^*,\tau^{*2})$) and then use the `qqplot` to compare them:
```{r lab5.4, echo=TRUE}
probs=seq(0,1,length=100)

# empirical quantiles
e_q <- quantile(sample,probs = probs)

#theoretical quantiles
t_q <- qnorm(probs,mu_star,sd_star)
{
qqplot(e_q,t_q,main="qq-plot",xlab="Empirical quantiles",ylab="Theoretical quantiles",ylim=c(1,4))
lines(e_q,e_q,col=2,lwd=2)
}
```
From both the histogram and the qq-plot I can see that the simulated values give us a good approximation of the theoretical posterior distribution.

## Ex. 6
Launch the following line of R code:
```R
posterior <- as.array(fit)
```
Use now the `bayesplot` package. Read the help and produce for this example, using the object posterior, the following plots:

- posterior intervals.

- posterior areas.

- marginal posterior distributions for the parameters.

Quickly comment.

---

Just as a reminder, the model we're fitting is the following:
$$
  Y \sim \mathcal{N}(\theta,\sigma^2)\\
  \theta \sim \mathcal{N}(\mu,\tau^2)
$$
where the hyperparametrs are fixed to $\sigma^2=2,\mu=7,\tau^2=2$.

Furthermore we'll attempt to deduce the posterior probability of $\theta$ from a sample of size 10 drawn $\mathcal{N}(2,2)$ by means of MCMC simulation. 

We first need to import the required library and run the stan HMC simulation to retrieve the data to plot:

```{r lab6, echo=TRUE}
library(rstan)
library(bayesplot)

#input values
theta_sample <- 2  #true mean
sigma2 <- 2        #likelihood variance
n <- 10            #sample size
mu <- 7            #prior mean
tau2 <- 2          #prior variance

#generate some data
set.seed(123)
y <- rnorm(n,theta_sample, sqrt(sigma2))

# launch Stan model
data<- list(N=n, y=y, sigma =sqrt(sigma2), mu = mu, tau = sqrt(tau2))
fit <- stan(file="3/normal.stan", data = data, chains = 4, iter=2000)
# retrieve the array of posterior samples from the stan HMC simulation 
posterior <- as.array(fit)

# There's only one parameter (which is theta). "lp__" is the log-posterior.
dimnames(posterior)

#print interval
mcmc_intervals(posterior, pars="theta",
               prob = 0.5,
               prob_outer = 0.9)
#print area
mcmc_areas(posterior, pars="theta",
               prob = 0.5,
               prob_outer = 0.9)

#marginal posterior
mcmc_dens(posterior, pars="theta")

```

The three graphs produced depict the same posterior estimates for the (only) parameter $\theta$. We also added a simple plot of the theoretical posterior distribution in the end for comparison: we can see that the simulated estimate is pretty close.

## Ex. 7

Suppose you receive $n=15$ phone calls in a day and you want to build a model to asses their average length. Your likelihood for each call is $y_i \sim \text{Poisson}(\lambda)$. Now, you have to choose the prior $\pi(\lambda)$. Please, tell which of these prior is adequate to describe the problem and provide a short motivation for each of them:

1. $\pi(\lambda)=\text{Beta}(4,2)$;

2. $\pi(\lambda)=\text{Normal}(1,2)$;

3. $\pi(\lambda)=\text{Gamma}(4,2)$;

Now compute your posterior as $\pi(\lambda|y)\propto L(\lambda;y)\pi(\lambda)$ for the selected prior. If your first choice was correct you will be able to compute it analitically.

---

Supposing that the length of the call is measured in minutes, let's plot and discuss which of the three distributions is suitable for our problem:

1. Beta(4,2)
```{r lab7.1, echo=TRUE}
x = seq(0,1, length=1000)
plot(x, dbeta(x, 4, 2), ylab="density", type ="l", lwd=1, col=4)
```
The Beta distribution is not suitable for our problem, the main reason is the fact that it is defined only for $x\in [0,1]$ so the mean length of the phone calls we receive could only be between 0 and 1 minute, which is very unlikely to be real.

2. Normal(1,2)
```{r lab7.2, echo=TRUE}
x <- seq(-10, 10, length=100)
plot(x, dnorm(x, 1, 2), type="l", lwd=1, col=4)
```
The normal distribution is not suitable for our problem, since it makes no sense that there could be negative values for the mean length of our phone calls.

3. Gamma(4,2)
```{r lab7.3, echo=TRUE}
x <- seq(0, 10, length=100)
plot(x, dgamma(x, 4, 2), type="l", lwd=1, col=4)
```
The gamma distribution is a suitable solution for our problem since it is defined for positive values and the possibile values for the mean lenght of the phone calls make sense.

Our data $\bar{y}=y_1,\dots,y_{n}$ are Poisson i.i.d. so the likelihood function will be:

$$L(\lambda|\bar{y})=\prod_{i=1}^{n} \frac{\mathrm{e}^{-\lambda} \lambda^{x_i}}{x_i!}=\frac{\mathrm{e}^{-n\lambda} \lambda^{\sum_{i=1}^{n}x_i}}{\prod_{i=1}^{n}(x_i!)}$$

The prior function will be:

$$p(\lambda)=\frac{\beta^{\alpha}}{\Gamma(\alpha)} \lambda^{\alpha-1} \mathrm{e}^{-\beta \lambda}$$

So we can analitically compute the posterior as:
$$\pi(\lambda|{y}) \propto L(\lambda;y)  \pi(\lambda) = \frac{\mathrm{e}^{-n\lambda} \lambda^{\sum_{i=1}^{n}x_i}}{\prod_{i=1}^{n}(x_i!)} \frac{\beta^{\alpha}}{\Gamma(\alpha)} \lambda^{\alpha-1} \mathrm{e}^{-\beta \lambda}$$
we can ignore the terms that do not depend on $\lambda$ so we obtain:
$$\pi(\lambda|{y}) \propto \mathrm{e}^{-n\lambda} \lambda^{\sum_{i=1}^{n}x_i} \lambda^{\alpha-1} \mathrm{e}^{-\beta \lambda} = \lambda^{(\alpha+ \sum_{i=1}^{n}x_i)-1} \mathrm{e}^{-(n+\beta)\lambda}$$
so the posterior distribution will be a Gamma$(\alpha+\sum_{i=1}^{n}y_i, n+\beta)$.

```{r lab7.4, echo=TRUE}
#true lambda
lambda <- 2
#sample size
n <- 15
#prior alpha
alpha <- 4
#prior beta
beta <- 2

#generate the data
set.seed(1234)
y <- rpois(n,lambda)

#posterior alpha
alpha_star <- sum(y)+alpha

#posterior beta
beta_star <- n+beta

lik_pois<- function(data, theta){
  n <- length(data)
  lambda <- theta
  dgamma(lambda, shape =sum(data)+1, scale=1/n)
}

lik_pois_v <- Vectorize(lik_pois, "theta")

curve(lik_pois_v(theta=x, data=y), xlim=c(0,6), lty=2, lwd=2, col="black", ylim=c(0,1.5), ylab="density", xlab=expression(lambda), cex.lab=2)
curve(dgamma(x, alpha, beta), xlim=c(0,6), col="red", lty=1,lwd=2,  add =T)
curve(dgamma(x, alpha_star, beta_star), 
      xlab=expression(lambda), ylab="", col="blue", lwd=2, add=T)
legend(4, 1, c("Prior", "Likelihood", "Posterior"),
       c("red", "black", "blue", "blue" ), lty=c(1,2,1),lwd=c(1,1,2), cex=0.8)
```

## Ex. 8

Follow the instructions to download and install the `rstan` library. Once you did it successfully, open the file model called "biparametric.stan", and replace the line:

`target+=cauchy_lpdf(sigma|0,2.5);`

with the following one:  

`target+=uniform_lpdf(sigma|0.1,10)`  

Which prior are you now assuming for your parameter $\sigma$? Reproduce the same plots as above and briefly comment.

---

```{r lab8, echo=TRUE}
library("bayesplot")
library("rstanarm")
library("ggplot2")
library("rstan")

#input values

#true mean
theta_sample <- 2
#likelihood variance
sigma2 <- 2
#sample size
n <- 10

#generate some data
set.seed(123)
y <- rnorm(n,theta_sample, sqrt(sigma2))

#launch biparametric Stan model

data3<- list(N=n, y=y, a=-10, b=10)
fit3 <- stan(file="3/biparametric2.stan", data = data3, chains = 4, iter=2000,
             refresh=-1)

#extract stan output for biparametric model

sim3 <- extract(fit3)
posterior_biv <- as.matrix(fit3)

theta_est <- mean(sim3$theta)
sigma_est <- mean(sim3$sigma)
c(theta_est, sigma_est)
traceplot(fit3, pars=c("theta", "sigma"))

plot_title <- ggtitle("Posterior distributions",
                      "with medians and 80% intervals")

mcmc_areas(posterior_biv, 
           pars = c("theta","sigma"), 
           prob = 0.8) + plot_title
```

Instead of using a half Cauchy distribution we are using a uniform distribution between 0.1 and 10. Both the trace plots converge but the values are slighlty different than before: $\theta$ goes from 2.1051 to 2.1089 and sigma goes from 1.5097 to 1.5681.


## Ex. 9

Reproduce the first plot above for the soccer goals, but this time by replacing prior 1 with a $\textit{Gamma(2,4)}$. Then, compute the final Bayes factor matrix `BF_matrix` with this new prior and the other ones unchanged, and comment. Is still prior 2 favorable over all the others?

---

The definition of the functions for the likelihood and for the priors remain the same because I change just the parameters of one of the priors and not the shape.

```{r lab9, echo=TRUE}
library(LearnBayes)

data(soccergoals)

y <- soccergoals$goals

#write the likelihood function via the gamma distribution
lik_pois<- function(data, theta){
  n <- length(data)
  lambda <- exp(theta)
  dgamma(lambda, shape =sum(data)+1, scale=1/n)
}

prior_gamma <- function(par, theta){
  lambda <- exp(theta) #why??
  dgamma(lambda, par[1], rate=par[2])*lambda  
}

prior_norm <- function(npar, theta){
  lambda=exp(theta)  
  (dnorm(theta, npar[1], npar[2]))
  
}


lik_pois_v <- Vectorize(lik_pois, "theta")
prior_gamma_v <- Vectorize(prior_gamma, "theta")
prior_norm_v <- Vectorize(prior_norm, "theta")
```

Now I can plot the likelihood and the priors (now the first prior is $\lambda \sim Gamma(2,4)$) as functions of $\theta=\log\lambda$:

```{r lab9.2, echo=TRUE}
#likelihood
curve(lik_pois_v(theta=x, data=y), xlim=c(-3,4), xlab=expression(theta), ylab = "density", lwd =2 )
#prior 1
curve(prior_gamma_v(theta=x, par=c(2, 4)), lty =2, col="red", add = TRUE, lwd =2)
#prior 2 
curve(prior_norm_v(theta=x, npar=c(1, .5)), lty =3, col="blue", add =TRUE, lwd=2)
#prior 3 
curve(prior_norm_v(theta=x, npar=c(2, .5)), lty =4, col="green", add =TRUE, lwd =2)
#prior 4 
curve(prior_norm_v(theta=x, npar=c(1, 2)), lty =5, col="violet", add =TRUE, lwd =2)
legend(1.88, 1.9, c("Lik.", "Ga(2,4)", "N(1, 0.25)", "N(2,0.25)","N(1, 4)" ),
       lty=c(1,2,3,4,5), col=c("black", "red", "blue", "green", "violet"),lwd=2, cex=0.9)
```
Now to compare different priors I have to compute the Bayes factors:
$$
BF_{ij}=\frac{m_i(y)}{m_j(y)}
$$
where $m_i(y)$ and $m_j(y)$ are the marginal likelihoods. To simplify computation I work with log posteriors: 
$$
log(\pi(\theta|y)) \propto l(\theta;y)+log(\pi(\theta))
$$
and I use the `Laplace` function to compute the log marginal likelihoods.

```{r lab9.3, echo=TRUE}
logpoissongamma <- function(theta, datapar){
  data <- datapar$data
  par <- datapar$par
  lambda <- exp(theta)
  log_lik <- log(lik_pois(data, theta))
  log_prior <- log(prior_gamma(par, theta))
  return(log_lik+log_prior)
}

logpoissongamma.v <- Vectorize( logpoissongamma, "theta")


logpoissonnormal <- function( theta, datapar){
  data <- datapar$data
  npar <- datapar$par
  lambda <- exp(theta)
  log_lik <- log(lik_pois(data, theta))
  log_prior <- log(prior_norm(npar, theta))
  return(log_lik+log_prior)
}  
logpoissonnormal.v <- Vectorize( logpoissonnormal, "theta")

datapar <- list(data=y, par=c(2, 4))
fit1 <- laplace(logpoissongamma, .5, datapar)
datapar <- list(data=y, par=c(1, .5))
fit2 <- laplace(logpoissonnormal, .5, datapar)
datapar <- list(data=y, par=c(2, .5))
fit3 <- laplace(logpoissonnormal, .5, datapar)
datapar <- list(data=y, par=c(1, 2))
fit4 <- laplace(logpoissonnormal, .5, datapar)

postmode <- c(fit1$mode, fit2$mode, fit3$mode, fit4$mode )
postsds <- sqrt(c(fit1$var, fit2$var, fit3$var, fit4$var))
logmarg <- c(fit1$int, fit2$int, fit3$int, fit4$int)
cbind(postmode, postsds, logmarg)

BF_matrix <- matrix(1, 4,4)
for (i in 1:3){
  for (j in 2:4){
    BF_matrix[i,j]<- exp(logmarg[i]-logmarg[j])
    BF_matrix[j,i]=(1/BF_matrix[i,j]) 
  }
}

round_bf <- round(BF_matrix,3)
round_bf
```

I can see that the posterior obtained with Prior1 has a lower mode w.r.t. the others because in more influenced from the new prior which has a lower mean than the likelihood. 

From the Bayes factor matrix I can compare different priors. Of course just the first row and the first column changed because just prior 1 has been modified. 

I can see that the second prior is still the preferable one because it's favored over Prior3 and Prior4 as before but it's also favored over new prior1, indeed the bayes factor $BF[2,1]=6.38$ says that Prior2 is 6.38 times more preferable than Prior1. 

Prior3 is still the less preferable one because all bayes factors in the third row are smaller or equal than 1.

The previous prior 1 was really similar to prior 2 and the last was preferable but $BF[2,1]$ was just 1.281 while with this new prior 1, which is really different from prior 2 but also from the likelihood, $BF[2,1]$ is 6.380 (almost 5 times the previous value). This means that prior 2 is preferable over the new prior 1 even strongly than it was over the old prior 1.

Now prior 1 is no more preferable over prior 4 as it was before because $BF[1,4]<1$. It's still preferable over prior 3 but less strongly ($BF[1,3]$ has changed from 35.635 to 7.156). 

## Ex. 10
Let `y=(1,0,0,1,0,0,0,0,0,1,0,0,1,0)` collect the results of tossing $n=14$ times an unfair coin, where 1 denotes heads and 0 tails, and $p=\text{Prob}(y_i=1)$.
Looking at the Stan code for the other models, write a short Stan Beta-Binomial model, where $p$ has a $\text{Beta}(a,b)$ prior with $a=3$, $b=3$.

- Extract the posterior distribution with the function `extract()`

- produce some plots with the bayesplot package and comment.

- compute analitically the posterior distribution and compare it with the Stan distribution.

---

The model we'll consider now is the following:
$$
  Y \sim \text{Bern}(\theta) = \theta^y (1-\theta)^{1-y}\\
  \theta \sim \text{Beta}(a,b) = \frac{\theta^{a-1} (1-\theta)^{b-1}}{B(a,b)}
$$
where we used $\theta$ instead of $p$ (to avoid notation clashing in the following demonstration) and $B(a,b)= \frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}$ is the Beta function.

We may compute the formula for the posterior explicitly:
$$\begin{align}
p(\theta|y_1,...,y_n) &\propto p(y_1,...,y_n|\theta) p(\theta)\\
&= \left[\prod_{i=1}^n \theta^{y_i} (1-\theta)^{1-y_i} \right]  \frac{\theta^{a-1} (1-\theta)^{b-1}}{B(a,b)} \\
&\propto \theta^{(a+\sum_{i}y_i)-1} (1-\theta)^{(b + n-\sum_{i}y_i) - 1}\\
&\propto \text{Beta}(a+\sum_{i=1}^n y_i, b + n-\sum_{i}y_i)
\end{align}$$

therefore in our specific case the expected posterior distribution for $\theta$ is a $\text{Beta}(7, 13)$.

Let's check this result experimentally by running a stan HMC simulation.
The stan model used is the following:

```
data{
  int N;
  int y[N];
  real<lower=0> a;
  real<lower=0> b;
}
parameters{
  real<lower=0, upper=1> theta;
}
model{
  target+=bernoulli_lpmf(y|theta);
  target+=beta_lpdf(theta|a,b);
}

```

(NOTE: We chose to model $Y$ using a Bernoulli distribution since it is equivalent to using a Binomial distribution with number of total trials fixed to 1.  We also checked that using said binomial formulation in the stan model yielded the same results.)

```{r lab10, echo=TRUE}
#beta params
a <- 3  
b <- 3

#samples
y <- c(1,0,0,1,0,0,0,0,0,1,0,0,1,0)
n <- length(y)

# launch Stan model
data<- list(N=n, y=y, a=a, b=b)
fit <- stan(file="3/bernbeta.stan", data = data, chains = 4, iter=2000)

# retrieve the array of posterior samples from the stan HMC simulation 
posterior <- as.array(fit)

# There's only one parameter (which is theta). "lp__" is the log-posterior.
dimnames(posterior)

#print interval
mcmc_intervals(posterior, pars="theta",
               prob = 0.5,
               prob_outer = 0.9)
               
#print area
mcmc_areas(posterior, pars="theta",
               prob = 0.5,
               prob_outer = 0.9)

#marginal posterior
mcmc_dens(posterior, pars="theta")

#theoretical (marginal) posterior plot
a_star <- 7
b_star <- 13
hist(posterior[,,1], probability = TRUE, breaks = 50,
     main="Samples histogram vs theoretical posterior",
     xlab=expression(theta), ylab="posterior")
curve(dbeta(x, a_star, b_star),
      xlim=c(0,1), col="red", lwd=1,
      cex.lab=2,
      add = TRUE)

```

# Exercises from LEC

## Ex 1

Compute the bootstrap-based confidence interval for the score dataset using the studentized method.

---

The bootstrap-based confidence interval with the studentized method has the form
$$
(\hat{\psi}- \text{SE}(\hat{\psi})z_{0.975}^*,\hat{\psi}- \text{SE}(\hat{\psi})z_{0.025}^* )
$$
where $z_{0.975}^*$ and $z_{0.025}^*$ are the bootstrap quantiles of $z_{1}^*$,..., $z_{B}^*$ for any $b=1,...,B$ (indices of resampling with replacement) and
$$z_{b}^*=\frac{\hat{\psi}^{*b}-\hat{\psi}}{\text{SE}(\hat{\psi}^{*b})}$$

Since this computation needs an explicit estimate of $\text{SE}(\hat{\psi}^*)$ for each bootstrap sample, we employ the jackknife within each bootstrap sample: hence, we define our statistic of interest (the eigenratio statistic, let's call it psi_fun), and we implement a for cicle in order to sample our statistic and do the explicit computation of the jackknife estimate of the standard error (SE_jack) within each bootstrap sample. Then we define the quantiles and the confidence interval:  

```{r lec1, echo=TRUE} 
library(boot)
library(knitr)
score <- read.table("https://web.stanford.edu/~hastie/CASI_files/DATA/student_score.txt", header=TRUE)
#our statistic of interest
psi_fun <- function(data,idx){
  n <- nrow(data)
  d <- data[idx,]
  eig <- eigen(cor(d))$values
  v <- max(eig) / sum(eig)
  psi_jack <- rep(0,n)
  for(j in 1:n){
    e <- eigen(cor(d[-j,]))$values
    psi_jack[j] <- max(e)/sum(e)
  } 
  SE_jack <- sqrt(((n - 1)/n) * sum((psi_jack - mean(psi_jack))^2))
  return(c(v,SE_jack))
}

#let's apply the statistic to our data
psi_obs <- psi_fun(score)

n <- nrow(score); B <- 10^4
zeta_vect <- rep(0, B)
psi_vect <- matrix(data=0,nrow=B,ncol=2)

#let's compute psi and zeta

for(i in 1:B) {
  ind <- sample(1:n, n, replace = TRUE)
  psi_vect[i,] <- psi_fun(score,ind)
  zeta_vect[i] <- (psi_vect[i,1]-psi_obs[1])/psi_vect[i,2]
}

zeta <- quantile(zeta_vect, prob=c(0.975, 0.025))

#let's compute the confidence interval

SE <- sd(psi_vect[,1])
stud_ci <- psi_obs[1] - SE*zeta
stud_ci

library(ggplot2)
library(MASS)

hist.scott(psi_vect[,1], main = "studentized confidence interval")
abline(v = psi_obs, col = 2)
mtext(expression(psi[obs]), 1, at=psi_obs, col="red")
abline(v=stud_ci[1],col=5)
abline(v=stud_ci[2],col=5)

```

Compared with the quantile-method interval, it is wider on the right side and approximately the same on the left side, while compared with the basic-method interval it is shorted on the left side and approximately the same on the right side.

## Ex. 2

Compute the bootstrap-based confidence interval for the score dataset using the boot package.

---

We just need to call the two functions `boot` and `boot.ci` from the `boot` package. In our case, the `boot.ci` function will receive the type `stud` as third argument; the implementation of this type requires the estimation of $\text{SE}_{jack}$, that's why we have inserted it in the second column of the `psi_func` when we've defined it in exercise 1.

```{r lec2, echo=TRUE}
a <- boot(score,psi_fun,R=10000)
boot.ci(a,conf=0.95,type="stud")
```

The Studentized confidence interval is slightly shorted than the one computed in the previous exercise.

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.